{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fossil-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wired-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodings with -1 and without one hot encoding\n",
    "train_1 = pd.read_csv(\"train_1encoded.csv\")\n",
    "train_1['SettlementProcess'].replace({'Card':0, 'Bank':1, 'Check':2, 'Electronic':3}, inplace=True)\n",
    "test_1  = pd.read_csv(\"test_1encoded.csv\")\n",
    "test_1['SettlementProcess'].replace({'Card':0, 'Bank':1, 'Check':2, 'Electronic':3}, inplace=True)\n",
    "\n",
    "# Encodings without -1 and with one hot encoding\n",
    "train_2 = pd.read_csv(\"train_encoded_noh.csv\")\n",
    "train_2['SettlementProcess'].replace({'Card':0, 'Bank':1, 'Check':2, 'Electronic':3}, inplace=True)\n",
    "test_2  = pd.read_csv(\"test_encoded_noh.csv\")\n",
    "test_2['SettlementProcess'].replace({'Card':0, 'Bank':1, 'Check':2, 'Electronic':3}, inplace=True)\n",
    "\n",
    "# Trivial encoding based on prev without 1 and one hot encoding is done\n",
    "train_3 = pd.read_csv(\"train_encoded.csv\")\n",
    "test_3  = pd.read_csv(\"test_encoded.csv\")\n",
    "\n",
    "# Introduce few new features train_1  ----->\n",
    "\n",
    "train_4 = train_1.copy()\n",
    "train_4['G/Q'] = train_4['GrandPayment']/train_4['QuarterlyPayment']\n",
    "train_4['G/S'] = train_4['GrandPayment']/(train_4['ServiceSpan']+3)\n",
    "train_4['Q/S'] = train_4['QuarterlyPayment']/(train_4['ServiceSpan']+3)\n",
    "train_4['Services_add'] = train_4['MobileService']+train_4['4GService']+ train_4['CyberProtection']+ train_4['HardwareSupport']+train_4['TechnicalAssistance'] + train_4['FilmSubscription']\n",
    "train_4['SAMT'] = train_4['sex']*(1-train_4['Aged'])*(1-train_4['Married']) +train_4['TotalDependents']\n",
    "\n",
    "test_4 = test_1.copy()\n",
    "test_4['G/Q'] = test_4['GrandPayment']/test_4['QuarterlyPayment']\n",
    "test_4['G/S'] = test_4['GrandPayment']/(test_4['ServiceSpan'] +3)\n",
    "test_4['Q/S'] = test_4['QuarterlyPayment']/(test_4['ServiceSpan']+3)\n",
    "test_4['Services_add'] = test_4['MobileService']+test_4['4GService']+ test_4['CyberProtection']+ test_4['HardwareSupport']+test_4['TechnicalAssistance'] + test_4['FilmSubscription']\n",
    "test_4['SAMT'] = test_4['sex']*(1-test_4['Aged'])*(1-test_4['Married']) +test_4['TotalDependents']\n",
    "\n",
    "# Introduce few new features train_2  ----->\n",
    "\n",
    "train_5 = train_2.copy()\n",
    "train_5['G/Q'] = train_5['GrandPayment']/train_5['QuarterlyPayment']\n",
    "train_5['G/S'] = train_5['GrandPayment']/(train_5['ServiceSpan'] +1)\n",
    "train_5['Q/S'] = train_5['QuarterlyPayment']/(train_5['ServiceSpan']+1)\n",
    "train_5['Services_add']= train_5['MobileService']+train_5['4GService']+ train_5['CyberProtection']+ train_5['HardwareSupport']+train_5['TechnicalAssistance'] + train_5['FilmSubscription']\n",
    "train_5['SAMT'] = train_5['sex']*(1-train_5['Aged'])*(1-train_5['Married']) +train_5['TotalDependents']\n",
    "\n",
    "test_5 = test_2.copy()\n",
    "test_5['G/Q'] = test_5['GrandPayment']/test_5['QuarterlyPayment']\n",
    "test_5['G/S'] = test_5['GrandPayment']/(test_5['ServiceSpan']+1)\n",
    "test_5['Q/S'] = test_5['QuarterlyPayment']/(test_5['ServiceSpan'] +1)\n",
    "test_5['Services_add'] = test_5['MobileService']+test_5['4GService']+ test_5['CyberProtection']+ test_5['HardwareSupport']+test_5['TechnicalAssistance'] + test_5['FilmSubscription']\n",
    "test_5['SAMT'] = test_5['sex']*(1-test_5['Aged'])*(1-test_5['Married']) +test_5['TotalDependents']\n",
    "\n",
    "# Introduce few new features train_3  ----->\n",
    "\n",
    "train_6 = train_3.copy()\n",
    "train_6['G/Q'] = train_6['GrandPayment']/train_6['QuarterlyPayment']\n",
    "train_6['G/S'] = train_6['GrandPayment']/(train_6['ServiceSpan']+1)\n",
    "train_6['Q/S'] = train_6['QuarterlyPayment']/(train_6['ServiceSpan']+1)\n",
    "# train_6['Services_add'] = train_6['MobileService']+train_6['4GService']+ train_6['CyberProtection']+ train_6['HardwareSupport']+train_6['TechnicalAssistance'] + train_6['FilmSubscription']\n",
    "# train_6['SAMT'] = train_6['sex']*(1-train_6['Aged'])*(1-train_6['Married']) +train_6['TotalDependents']\n",
    "\n",
    "test_6 = test_3.copy()\n",
    "test_6['G/Q'] = test_6['GrandPayment']/test_6['QuarterlyPayment']\n",
    "test_6['G/S'] = test_6['GrandPayment']/(test_6['ServiceSpan']+1)\n",
    "test_6['Q/S'] = test_6['QuarterlyPayment']/(test_6['ServiceSpan']+1)\n",
    "# test_6['Services_add'] = test_6['MobileService']+test_6['4GService']+ test_6['CyberProtection']+ test_6['HardwareSupport']+test_6['TechnicalAssistance'] + test_6['FilmSubscription']\n",
    "# test_6['SAMT'] = test_6['sex']*(1-test_6['Aged'])*(1-test_6['Married']) +test_6['TotalDependents']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "backed-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "### General ###\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(\"./tfms\")\n",
    "\n",
    "### Data Wrangling ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "### Data Visualization ###\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "### Machine Learning ###\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "### Make prettier the prints ###\n",
    "from colorama import Fore\n",
    "c_ = Fore.CYAN\n",
    "m_ = Fore.MAGENTA\n",
    "r_ = Fore.RED\n",
    "b_ = Fore.BLUE\n",
    "y_ = Fore.YELLOW\n",
    "g_ = Fore.GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "commercial-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from catboost import CatBoostRegressor , CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accessible-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "express-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thermal-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = \"rankgauss\"   # boxcox , norm ,minmax , rankgauss\n",
    "variance_threshould = None\n",
    "decompo = \"no\" #\"PCA\"\n",
    "ncompo = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "casual-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(df,dum_cols,val = 0):\n",
    "    if val == 0:\n",
    "        return df\n",
    "    return pd.get_dummies(df, prefix=None, prefix_sep='_', dummy_na=False, columns=dum_cols)\n",
    "\n",
    "def normalize(df):\n",
    "    return (df - df.mean(0) )/df.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "successful-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_2.copy()\n",
    "test_df  = test_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stunning-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_df.drop(['CustomerAttrition','ID'],axis=1)\n",
    "train_targets_scored = train_df['CustomerAttrition']\n",
    "\n",
    "test_features = test_df.drop(['ID'],axis=1)\n",
    "sample_submission = pd.read_csv('Sample Submission.csv')\n",
    "\n",
    "concat = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "create_dummy = 0                #0 if dont want dummies\n",
    "dum_cols = ['age', 'experience', 'married', 'house_ownership',\n",
    "       'car_ownership', 'profession', 'city', 'state', 'current_job_years',\n",
    "       'current_house_years']\n",
    "\n",
    "concat = get_dummies(concat,dum_cols,create_dummy)\n",
    "# norm_concat = normalize(concat)\n",
    "\n",
    "train_features = concat[:len(train_features)]\n",
    "test_features  = concat[len(train_features):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sapphire-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = concat.copy()\n",
    "cols_numeric = [feat for feat in list(data_all.columns)]\n",
    "if variance_threshould:\n",
    "    mask = (data_all[cols_numeric].var() >= variance_threshould).values\n",
    "    data_all = data_all[cols_numeric].loc[:, mask]\n",
    "    cols_numeric = [feat for feat in list(data_all.columns)]\n",
    "data_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vulnerable-thirty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                  0\n",
       "sex                    0\n",
       "Aged                   0\n",
       "Married                0\n",
       "TotalDependents        0\n",
       "ServiceSpan            0\n",
       "MobileService          0\n",
       "4GService              0\n",
       "CyberProtection        0\n",
       "HardwareSupport        0\n",
       "TechnicalAssistance    0\n",
       "FilmSubscription       0\n",
       "SettlementProcess      0\n",
       "QuarterlyPayment       0\n",
       "GrandPayment           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.isin([np.inf, -np.inf]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "better-theme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m Rank Gauss\n"
     ]
    }
   ],
   "source": [
    "def scale_minmax(col):\n",
    "    return (col - col.min()) / (col.max() - col.min())\n",
    "\n",
    "def scale_norm(col):\n",
    "    return (col - col.mean()) / col.std()\n",
    "\n",
    "if scale == \"boxcox\":\n",
    "    print(b_, \"boxcox\")\n",
    "    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n",
    "    trans = []\n",
    "    for feat in cols_numeric:\n",
    "        trans_var, lambda_var = stats.boxcox(data_all[feat].dropna() + 1)\n",
    "        trans.append(scale_minmax(trans_var))\n",
    "    data_all[cols_numeric] = np.asarray(trans).T\n",
    "    \n",
    "elif scale == \"norm\":\n",
    "    print(b_, \"norm\")\n",
    "    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_norm, axis = 0)\n",
    "    \n",
    "elif scale == \"minmax\":\n",
    "    print(b_, \"minmax\")\n",
    "    data_all[cols_numeric] = data_all[cols_numeric].apply(scale_minmax, axis = 0)\n",
    "    \n",
    "elif scale == \"rankgauss\":\n",
    "    ### Rank Gauss ###\n",
    "    print(b_, \"Rank Gauss\")\n",
    "    scaler = GaussRankScaler()\n",
    "    data_all[cols_numeric] = scaler.fit_transform(data_all[cols_numeric])\n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fifth-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "if decompo == \"PCA\":\n",
    "    print(b_, \"PCA\")\n",
    "    \n",
    "    pca_genes = PCA(n_components = ncompo,\n",
    "                    random_state = seed).fit_transform(data_all)\n",
    "    \n",
    "    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pcaHarsh-{i}\" for i in range(ncompo)])\n",
    "    data_all = pd.concat([data_all, pca_genes], axis=1)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tough-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mtrain_df.shape: \u001b[31m(6337, 15)\n",
      "\u001b[34mtest_df.shape: \u001b[31m(705, 14)\n",
      "\u001b[34mX_test.shape: \u001b[31m(705, 14)\n"
     ]
    }
   ],
   "source": [
    "train_df = data_all[: train_df.shape[0]]\n",
    "train_df = pd.concat([train_df,train_targets_scored],axis=1)\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "test_df = data_all[train_df.shape[0]: ]\n",
    "test_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "train_df = train_df.drop(['index'],axis=1)\n",
    "test_df = test_df.drop(['index'],axis=1)\n",
    "\n",
    "print(f\"{b_}train_df.shape: {r_}{train_df.shape}\")\n",
    "print(f\"{b_}test_df.shape: {r_}{test_df.shape}\")\n",
    "\n",
    "X_test = test_df.values\n",
    "print(f\"{b_}X_test.shape: {r_}{X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "familiar-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = accuracy_score(y_true, np.round(y_pred))\n",
    "    return 'f1_err', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "corporate-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _F1_eval(preds, labels):\n",
    "    t = np.arange(0, 1, 0.005)\n",
    "    f = np.repeat(0, 200)\n",
    "    results = np.vstack([t, f]).T\n",
    "    # assuming labels only containing 0's and 1's\n",
    "    n_pos_examples = sum(labels)\n",
    "    if n_pos_examples == 0:\n",
    "        raise ValueError(\"labels not containing positive examples\")\n",
    "\n",
    "    for i in range(200):\n",
    "        pred_indexes = (preds >= results[i, 0])\n",
    "        TP = sum(labels[pred_indexes])\n",
    "        FP = len(labels[pred_indexes]) - TP\n",
    "        precision = 0\n",
    "        recall = TP / n_pos_examples\n",
    "\n",
    "        if (FP + TP) > 0:\n",
    "            precision = TP / (FP + TP)\n",
    "\n",
    "        if (precision + recall > 0):\n",
    "            F1 = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            F1 = 0\n",
    "        results[i, 1] = F1\n",
    "    return (max(results[:, 1]))\n",
    "\n",
    "def F1_eval(preds, dtrain):\n",
    "    res = _F1_eval(preds, dtrain.get_label())\n",
    "    return 'f1_err', 1-res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "velvet-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "polyphonic-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m FOLDS:  \u001b[31m 1\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8276229362684182 f1: 0.6266820453671665\n",
      "Val acc: 0.7982954545454546 f1: 0.5477707006369426\n",
      "\u001b[34m FOLDS:  \u001b[31m 2\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8078835227272727 f1: 0.574685534591195\n",
      "Val acc: 0.8 f1: 0.5607476635514018\n",
      "\u001b[34m FOLDS:  \u001b[31m 3\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8160837919403515 f1: 0.5978260869565217\n",
      "Val acc: 0.8011363636363636 f1: 0.5833333333333333\n",
      "\u001b[34m FOLDS:  \u001b[31m 4\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8048996982069945 f1: 0.5647524752475248\n",
      "Val acc: 0.7982954545454546 f1: 0.5448717948717948\n",
      "\u001b[34m FOLDS:  \u001b[31m 5\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8191017219953843 f1: 0.6070188970304666\n",
      "Val acc: 0.7982954545454546 f1: 0.5617283950617284\n",
      "\u001b[34m FOLDS:  \u001b[31m 6\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8102254571276407 f1: 0.5851765618936747\n",
      "Val acc: 0.8011363636363636 f1: 0.5652173913043479\n",
      "\u001b[34m FOLDS:  \u001b[31m 7\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8183916208059648 f1: 0.6039488966318236\n",
      "Val acc: 0.8096590909090909 f1: 0.593939393939394\n",
      "\u001b[34m FOLDS:  \u001b[31m 8\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8116456595064797 f1: 0.5879611650485438\n",
      "Val acc: 0.7869318181818182 f1: 0.53125\n",
      "\u001b[34m FOLDS:  \u001b[31m 9\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Train acc: 0.8151961654535771 f1: 0.5988439306358381\n",
      "Val acc: 0.7982954545454546 f1: 0.5644171779141104\n",
      "OOF acc: 0.799116\n"
     ]
    }
   ],
   "source": [
    "scores_auc_all = []\n",
    "test_cv_preds = []\n",
    "\n",
    "NB_SPLITS = 9\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\n",
    "\n",
    "oof_preds = []\n",
    "oof_targets = []\n",
    "scores = []\n",
    "scores_auc = []\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "train_feat        = []\n",
    "train_labels      = []\n",
    "train_predictions = []\n",
    "\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# for mskf\n",
    "ms_tar = np.array(train_df)\n",
    "targets = train_df.CustomerAttrition.values\n",
    "train_it = train_df.drop(['CustomerAttrition'],axis=1)\n",
    "#####\n",
    "\n",
    "for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_it, ms_tar)):\n",
    "    print(b_,\"FOLDS: \", r_, fold_nb + 1)\n",
    "    print(g_, '*' * 60, c_)\n",
    "    \n",
    "    X_train, y_train = train_it.values[train_idx, :], np.array(targets).reshape(-1,1)[train_idx]\n",
    "    X_val, y_val = train_it.values[val_idx, :], np.array(targets).reshape(-1,1)[val_idx]\n",
    "    ### Model ###\n",
    "    \n",
    "    \n",
    "#     model = DecisionTreeClassifier(max_depth = 4,random_state=42)\n",
    "#     model = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='log2',subsample=0.9,random_state=42)\n",
    "    \n",
    "#     model = xgb.XGBClassifier(max_depth=6, learning_rate=0.05,silent=False, objective='binary:logistic',\n",
    "#                       booster='gbtree', n_jobs=8, nthread=4, gamma=0, min_child_weight=1, max_delta_step=0,\n",
    "#                       subsample=0.8, colsample_bytree=0.8, colsample_bylevel=1, reg_alpha=0, reg_lambda=1)\n",
    "    model = CatBoostClassifier(iterations=5000, depth=4, learning_rate=0.04 ,l2_leaf_reg=5, loss_function='CrossEntropy',verbose=False,early_stopping_rounds = 200)\n",
    "    model.fit(X_train,y_train,eval_set=(X_val, y_val),verbose_eval= False,use_best_model=True)\n",
    "\n",
    "\n",
    "    out = model.predict(X_train)\n",
    "    acc = accuracy_score(y_train, out)\n",
    "    f1  = f1_score(y_train,out)\n",
    "    print(f'Train acc: {acc} f1: {f1}')\n",
    "    \n",
    "    out = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, out)\n",
    "    f1  = f1_score(y_val,out)\n",
    "    print(f'Val acc: {acc} f1: {f1}')\n",
    "    \n",
    "    \n",
    "    ### Save OOF for CV ###\n",
    "    oof_preds.append(out)\n",
    "    oof_targets.append(y_val.reshape(-1,))\n",
    "    scores.append(f1)\n",
    "    \n",
    "    ### Predict on test ###\n",
    "    preds_test = model.predict_proba(X_test)[:,1:].reshape(-1)\n",
    "\n",
    "#     preds_test = model.predict(X_test)\n",
    "    test_cv_preds.append(preds_test)\n",
    "\n",
    "oof_preds_all = np.concatenate(oof_preds)\n",
    "oof_targets_all = np.concatenate(oof_targets)\n",
    "test_preds_all = np.stack(test_cv_preds)\n",
    "\n",
    "oof = accuracy_score(oof_targets_all,oof_preds_all)\n",
    "print('OOF acc: %f' % oof)\n",
    "\n",
    "sample_sub = pd.read_csv(\"Sample Submission.csv\")\n",
    "\n",
    "# Lets Take Weighted ensemble\n",
    "base = np.zeros((len(sample_sub)))\n",
    "for itr,prs in enumerate(test_preds_all):\n",
    "    base += (scores[itr]/np.sum(scores))*prs\n",
    "\n",
    "sample_sub['CustomerAttrition'] = 1*(base >= 0.5)\n",
    "\n",
    "sample_sub.replace({1:'Yes', 0:'No'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "subtle-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_t = test_df.copy()\n",
    "dummy_t['CustomerAttrition'] = 1*(base >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "according-horizontal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m FOLDS:  \u001b[31m 1\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7954545454545454 f1: 0.5263157894736842\n",
      "\u001b[34m FOLDS:  \u001b[31m 2\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7957446808510639 f1: 0.5384615384615384\n",
      "\u001b[34m FOLDS:  \u001b[31m 3\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8082386363636364 f1: 0.599406528189911\n",
      "\u001b[34m FOLDS:  \u001b[31m 4\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7997159090909091 f1: 0.5495207667731629\n",
      "\u001b[34m FOLDS:  \u001b[31m 5\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8153409090909091 f1: 0.59375\n",
      "\u001b[34m FOLDS:  \u001b[31m 6\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7940340909090909 f1: 0.5510835913312694\n",
      "\u001b[34m FOLDS:  \u001b[31m 7\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8082386363636364 f1: 0.5871559633027522\n",
      "\u001b[34m FOLDS:  \u001b[31m 8\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7840909090909091 f1: 0.525\n",
      "\u001b[34m FOLDS:  \u001b[31m 9\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8025568181818182 f1: 0.5696594427244582\n",
      "OOF acc: 0.800379\n"
     ]
    }
   ],
   "source": [
    "scores_auc_all = []\n",
    "test_cv_preds = []\n",
    "\n",
    "NB_SPLITS = 9\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\n",
    "\n",
    "oof_preds = []\n",
    "oof_targets = []\n",
    "scores = []\n",
    "scores_auc = []\n",
    "\n",
    "# for mskf\n",
    "ms_tar = np.array(train_df)\n",
    "targets = train_df.CustomerAttrition.values\n",
    "train_it = train_df.drop(['CustomerAttrition'],axis=1)\n",
    "#####\n",
    "\n",
    "for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_it, ms_tar)):\n",
    "    print(b_,\"FOLDS: \", r_, fold_nb + 1)\n",
    "    print(g_, '*' * 60, c_)\n",
    "    \n",
    "    X_train, y_train = train_it.values[train_idx, :], np.array(targets).reshape(-1,1)[train_idx]\n",
    "    X_train = np.vstack([X_train,np.array(dummy_t)[:,:-1]])\n",
    "    y_train = np.vstack([y_train,np.array(dummy_t)[:,-1].reshape(-1,1)])\n",
    "    \n",
    "    a = np.hstack([X_train,y_train])\n",
    "    X_train = a[:,:-1]\n",
    "    y_train = a[:,-1]\n",
    "    \n",
    "    X_val, y_val = train_it.values[val_idx, :], np.array(targets).reshape(-1,1)[val_idx]\n",
    "    ### Model ###\n",
    "    \n",
    "    \n",
    "#     model = DecisionTreeClassifier(max_depth = 4,random_state=42)\n",
    "#     model = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='log2',subsample=0.9,random_state=42)\n",
    "    \n",
    "#     model = xgb.XGBClassifier(max_depth=6, learning_rate=0.05,silent=False, objective='binary:logistic',\n",
    "#                       booster='gbtree', n_jobs=8, nthread=4, gamma=0, min_child_weight=1, max_delta_step=0,\n",
    "#                       subsample=0.8, colsample_bytree=0.8, colsample_bylevel=1, reg_alpha=0, reg_lambda=1)\n",
    "    model = CatBoostClassifier(iterations=5000, depth=4, learning_rate=0.04 ,l2_leaf_reg=5, loss_function='CrossEntropy',verbose=False,early_stopping_rounds = 200)\n",
    "    model.fit(X_train,y_train,eval_set=(X_val, y_val),verbose_eval= False,use_best_model=True)\n",
    "\n",
    "\n",
    "#     out = model.predict(X_train)\n",
    "#     acc = accuracy_score(y_train, out)\n",
    "#     f1  = f1_score(y_train,out)\n",
    "#     print(f'Train acc: {acc} f1: {f1}')\n",
    "    \n",
    "    out = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, out)\n",
    "    f1  = f1_score(y_val,out)\n",
    "    print(f'Val acc: {acc} f1: {f1}')\n",
    "    \n",
    "    \n",
    "    ### Save OOF for CV ###\n",
    "    oof_preds.append(out)\n",
    "    oof_targets.append(y_val.reshape(-1,))\n",
    "    scores.append(f1)\n",
    "    \n",
    "    ### Predict on test ###\n",
    "    preds_test = model.predict_proba(X_test)[:,1:].reshape(-1)\n",
    "\n",
    "#     preds_test = model.predict(X_test)\n",
    "    test_cv_preds.append(preds_test)\n",
    "\n",
    "oof_preds_all = np.concatenate(oof_preds)\n",
    "oof_targets_all = np.concatenate(oof_targets)\n",
    "test_preds_all = np.stack(test_cv_preds)\n",
    "\n",
    "\n",
    "oof = accuracy_score(oof_targets_all,oof_preds_all)\n",
    "print('OOF acc: %f' % oof)\n",
    "\n",
    "sample_sub = pd.read_csv(\"Sample Submission.csv\")\n",
    "\n",
    "# Lets Take Weighted ensemble\n",
    "base = np.zeros((len(sample_sub)))\n",
    "for itr,prs in enumerate(test_preds_all):\n",
    "    base += (scores[itr]/np.sum(scores))*prs\n",
    "\n",
    "sample_sub['CustomerAttrition'] = 1*(base >= 0.5)\n",
    "\n",
    "sample_sub.replace({1:'Yes', 0:'No'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fantastic-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_t = test_df.copy()\n",
    "dummy_t['CustomerAttrition'] = base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "divided-optimization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m FOLDS:  \u001b[31m 1\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.796875 f1: 0.5401929260450161\n",
      "\u001b[34m FOLDS:  \u001b[31m 2\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8070921985815603 f1: 0.5723270440251572\n",
      "\u001b[34m FOLDS:  \u001b[31m 3\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7982954545454546 f1: 0.5748502994011976\n",
      "\u001b[34m FOLDS:  \u001b[31m 4\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8011363636363636 f1: 0.5541401273885351\n",
      "\u001b[34m FOLDS:  \u001b[31m 5\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8068181818181818 f1: 0.5750000000000001\n",
      "\u001b[34m FOLDS:  \u001b[31m 6\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8011363636363636 f1: 0.5705521472392637\n",
      "\u001b[34m FOLDS:  \u001b[31m 7\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.8125 f1: 0.5950920245398772\n",
      "\u001b[34m FOLDS:  \u001b[31m 8\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7855113636363636 f1: 0.5353846153846155\n",
      "\u001b[34m FOLDS:  \u001b[31m 9\n",
      "\u001b[32m ************************************************************ \u001b[36m\n",
      "Val acc: 0.7954545454545454 f1: 0.558282208588957\n"
     ]
    }
   ],
   "source": [
    "scores_auc_all = []\n",
    "test_cv_preds = []\n",
    "\n",
    "NB_SPLITS = 9\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits = NB_SPLITS, random_state = 0, shuffle = True)\n",
    "\n",
    "oof_preds = []\n",
    "oof_targets = []\n",
    "scores = []\n",
    "scores_auc = []\n",
    "\n",
    "# for mskf\n",
    "ms_tar = np.array(train_df)\n",
    "targets = train_df.CustomerAttrition.values\n",
    "train_it = train_df.drop(['CustomerAttrition'],axis=1)\n",
    "#####\n",
    "\n",
    "for fold_nb, (train_idx, val_idx) in enumerate(mskf.split(train_it, ms_tar)):\n",
    "    print(b_,\"FOLDS: \", r_, fold_nb + 1)\n",
    "    print(g_, '*' * 60, c_)\n",
    "    \n",
    "    X_train, y_train = train_it.values[train_idx, :], np.array(targets).reshape(-1,1)[train_idx]\n",
    "    X_train = np.vstack([X_train,np.array(dummy_t)[:,:-1]])\n",
    "    y_train = np.vstack([y_train,np.array(dummy_t)[:,-1].reshape(-1,1)])\n",
    "    \n",
    "    a = np.hstack([X_train,y_train])\n",
    "    X_train = a[:,:-1]\n",
    "    y_train = a[:,-1]\n",
    "    \n",
    "    X_val, y_val = train_it.values[val_idx, :], np.array(targets).reshape(-1,1)[val_idx]\n",
    "    ### Model ###\n",
    "    \n",
    "    \n",
    "#     model = DecisionTreeClassifier(max_depth = 4,random_state=42)\n",
    "#     model = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='log2',subsample=0.9,random_state=42)\n",
    "    \n",
    "#     model = xgb.XGBClassifier(max_depth=6, learning_rate=0.05,silent=False, objective='binary:logistic',\n",
    "#                       booster='gbtree', n_jobs=8, nthread=4, gamma=0, min_child_weight=1, max_delta_step=0,\n",
    "#                       subsample=0.8, colsample_bytree=0.8, colsample_bylevel=1, reg_alpha=0, reg_lambda=1)\n",
    "    model = CatBoostClassifier(iterations=5000, depth=4, learning_rate=0.04 ,l2_leaf_reg=5, loss_function='CrossEntropy',verbose=False,early_stopping_rounds = 200)\n",
    "    model.fit(X_train,y_train,eval_set=(X_val, y_val),verbose_eval= False,use_best_model=True)\n",
    "\n",
    "\n",
    "#     out = model.predict(X_train)\n",
    "#     acc = accuracy_score(y_train, out)\n",
    "#     f1  = f1_score(y_train,out)\n",
    "#     print(f'Train acc: {acc} f1: {f1}')\n",
    "    \n",
    "    out = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, out)\n",
    "    f1  = f1_score(y_val,out)\n",
    "    print(f'Val acc: {acc} f1: {f1}')\n",
    "    \n",
    "    \n",
    "    ### Save OOF for CV ###\n",
    "    oof_preds.append(out)\n",
    "    oof_targets.append(y_val.reshape(-1,))\n",
    "    scores.append(f1)\n",
    "    \n",
    "    ### Predict on test ###\n",
    "    preds_test = model.predict_proba(X_test)[:,1:].reshape(-1)\n",
    "\n",
    "#     preds_test = model.predict(X_test)\n",
    "    test_cv_preds.append(preds_test)\n",
    "\n",
    "oof_preds_all = np.concatenate(oof_preds)\n",
    "oof_targets_all = np.concatenate(oof_targets)\n",
    "test_preds_all = np.stack(test_cv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "treated-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF acc: 0.800537\n"
     ]
    }
   ],
   "source": [
    "oof = accuracy_score(oof_targets_all,oof_preds_all)\n",
    "print('OOF acc: %f' % oof)\n",
    "\n",
    "sample_sub = pd.read_csv(\"Sample Submission.csv\")\n",
    "\n",
    "# Lets Take Weighted ensemble\n",
    "base = np.zeros((len(sample_sub)))\n",
    "for itr,prs in enumerate(test_preds_all):\n",
    "    base += (scores[itr]/np.sum(scores))*prs\n",
    "\n",
    "sample_sub['CustomerAttrition'] = 1*(base >= 0.5)\n",
    "\n",
    "sample_sub.replace({1:'Yes', 0:'No'}, inplace=True)\n",
    "\n",
    "dummy_t = test_df.copy()\n",
    "dummy_t['CustomerAttrition'] = 1*(base >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bottom-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv(\"submission_3timestrain.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-intention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "frozen-pilot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     571\n",
       "Yes    134\n",
       "Name: CustomerAttrition, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub.CustomerAttrition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "horizontal-cornell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971631205673759"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sample_sub.CustomerAttrition.values == pd.read_csv(\"match.csv\").CustomerAttrition.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-offense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-pixel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-retro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-player",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
